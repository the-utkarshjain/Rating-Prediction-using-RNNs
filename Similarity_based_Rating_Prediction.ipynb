{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OsRLAB-DmOWt"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = pd.read_csv('/content/drive/MyDrive/clean_data/train.csv')\n",
        "validation_dataset = pd.read_csv('/content/drive/MyDrive/clean_data/valid.csv')\n",
        "test_dataset = pd.read_csv('/content/drive/MyDrive/clean_data/test.csv')\n",
        "\n",
        "dataset = pd.concat([train_dataset, validation_dataset]) # merges train and validation dataset into one beacause we don't need a validation set for similarity-based rating prediction\n",
        "dataset.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_gOayKxnWju",
        "outputId": "8e1282c8-8552-4dc9-fd9c-180193833b66"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(175869, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "usersPerItem = defaultdict(set)\n",
        "itemsPerUser = defaultdict(set)\n",
        "reviewsPerUser = defaultdict(list)\n",
        "reviewsPerItem = defaultdict(list)\n",
        "ratingDict = {}"
      ],
      "metadata": {
        "id": "3svWmFuNobXm"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in dataset.iterrows():\n",
        "  item = row['item']\n",
        "  user = row['user']\n",
        "  rating = row['rating']\n",
        "\n",
        "  usersPerItem[item].add(user)\n",
        "  itemsPerUser[user].add(item)\n",
        "\n",
        "  ratingDict[(item, user)] = rating\n",
        "\n",
        "  reviewsPerUser[user].append({'item': item, 'rating': rating})\n",
        "  reviewsPerItem[item].append({'user': user, 'rating': rating})"
      ],
      "metadata": {
        "id": "plhK1en1o2uw"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratingMean = dataset['rating'].mean()"
      ],
      "metadata": {
        "id": "QByi6DSMujgV"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "userAverages = defaultdict(float)\n",
        "itemAverages = defaultdict(float)\n",
        "\n",
        "for u in itemsPerUser:\n",
        "    rs = [ratingDict[(i,u)] for i in itemsPerUser[u]]\n",
        "    userAverages[u] = sum(rs) / len(rs)\n",
        "    \n",
        "for i in usersPerItem:\n",
        "    rs = [ratingDict[(i,u)] for u in usersPerItem[i]]\n",
        "    itemAverages[i] = sum(rs) / len(rs)"
      ],
      "metadata": {
        "id": "oR-cHDFdxppD"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing similarity functions\n",
        "\n",
        "TODO:\n",
        "1.   Implement Cosine and Pearson Similarity functions and check the MSE\n",
        "\n"
      ],
      "metadata": {
        "id": "vahjgY0N4Ppe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard(s1, s2):\n",
        "  num = len(s1.intersection(s2))\n",
        "  den = len(s1.union(s2))\n",
        "\n",
        "  if den == 0: \n",
        "    return 0\n",
        "  return num/den"
      ],
      "metadata": {
        "id": "Za6XRLwIwx0g"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Similarity-based Rating Estimation"
      ],
      "metadata": {
        "id": "YfBJorNy4WTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MSE(predictions, labels):\n",
        "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
        "    return sum(differences) / len(differences)"
      ],
      "metadata": {
        "id": "W05IqlRE2Eh7"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predictRating(user, item):\n",
        "  ratings = []\n",
        "  similarities = []\n",
        "\n",
        "  for d in reviewsPerUser[user]:\n",
        "    item2 = d['item']\n",
        "\n",
        "    if item2 == item: continue\n",
        "\n",
        "    if(itemAverages[item2] != 0):\n",
        "      ratings.append(d['rating'] - itemAverages[item2])\n",
        "    else:\n",
        "      ratings.append(d['rating'] - ratingMean)\n",
        "\n",
        "    similarities.append(jaccard(usersPerItem[item2], usersPerItem[item]))\n",
        "\n",
        "  if (sum(similarities) > 0):\n",
        "    weightedRatings = [(x*y) for x,y in zip(ratings,similarities)]\n",
        "    if(itemAverages[item] != 0):\n",
        "      return itemAverages[item] + sum(weightedRatings) / sum(similarities)\n",
        "    else:\n",
        "      return ratingMean + sum(weightedRatings) / sum(similarities)\n",
        "  else:\n",
        "    if(itemAverages[item] != 0):\n",
        "      return itemAverages[item]\n",
        "    else:\n",
        "      return ratingMean"
      ],
      "metadata": {
        "id": "ncU7b1WKvoSH"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alwaysMean = [] # when you always predict mean\n",
        "predictions = []\n",
        "actual = []\n",
        "\n",
        "for index, row in test_dataset.iterrows():\n",
        "  user = row['user']\n",
        "  item = row['item']\n",
        "  actual_rating = row['rating']\n",
        "  predicted_rating = predictRating(user, item)\n",
        "\n",
        "  actual.append(actual_rating)\n",
        "  alwaysMean.append(ratingMean)\n",
        "  predictions.append(predicted_rating)"
      ],
      "metadata": {
        "id": "OXPR-XVN1dE1"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MSE(alwaysMean, actual)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odg5FNWD2LZ2",
        "outputId": "901c2179-d11b-46fc-89c7-8f21b0c62247"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.2220352124670564"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MSE(predictions, actual)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zWXG_7i2G6R",
        "outputId": "4ef3aa63-65c2-46a8-d754-a9ee8fafc1d2"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.9498775185780508"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    }
  ]
}