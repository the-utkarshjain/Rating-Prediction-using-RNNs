{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVasTECjHRxn",
        "outputId": "62784a6a-eb64-44f9-e515-2c2497a2d964"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OsRLAB-DmOWt"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = pd.read_csv('/content/drive/Shareddrives/CSE258/clean_data/train.csv')\n",
        "validation_dataset = pd.read_csv('/content/drive/Shareddrives/CSE258/clean_data/valid.csv')\n",
        "test_dataset = pd.read_csv('/content/drive/Shareddrives/CSE258/clean_data/test.csv')\n",
        "\n",
        "dataset = pd.concat([train_dataset, validation_dataset]) # merges train and validation dataset into one beacause we don't need a validation set for similarity-based rating prediction\n",
        "dataset.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_gOayKxnWju",
        "outputId": "d296a2ec-cf04-4e3d-d49d-d871661d2117"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(175869, 6)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "usersPerItem = defaultdict(set)\n",
        "itemsPerUser = defaultdict(set)\n",
        "reviewsPerUser = defaultdict(list)\n",
        "reviewsPerItem = defaultdict(list)\n",
        "ratingDict = {}"
      ],
      "metadata": {
        "id": "3svWmFuNobXm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for index, row in dataset.iterrows():\n",
        "  item = row['item']\n",
        "  user = row['user']\n",
        "  rating = row['rating']\n",
        "\n",
        "  usersPerItem[item].add(user)\n",
        "  itemsPerUser[user].add(item)\n",
        "\n",
        "  ratingDict[(item, user)] = rating\n",
        "\n",
        "  reviewsPerUser[user].append({'item': item, 'rating': rating})\n",
        "  reviewsPerItem[item].append({'user': user, 'rating': rating})"
      ],
      "metadata": {
        "id": "plhK1en1o2uw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ratingMean = dataset['rating'].mean()"
      ],
      "metadata": {
        "id": "QByi6DSMujgV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "userAverages = defaultdict(float)\n",
        "itemAverages = defaultdict(float)\n",
        "\n",
        "for u in itemsPerUser:\n",
        "    rs = [ratingDict[(i,u)] for i in itemsPerUser[u]]\n",
        "    userAverages[u] = sum(rs) / len(rs)\n",
        "    \n",
        "for i in usersPerItem:\n",
        "    rs = [ratingDict[(i,u)] for u in usersPerItem[i]]\n",
        "    itemAverages[i] = sum(rs) / len(rs)"
      ],
      "metadata": {
        "id": "oR-cHDFdxppD"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implementing similarity functions\n",
        "\n",
        "TODO:\n",
        "1.   Implement Cosine and Pearson Similarity functions and check the MSE\n",
        "\n"
      ],
      "metadata": {
        "id": "vahjgY0N4Ppe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def jaccard(s1, s2):\n",
        "  num = len(s1.intersection(s2))\n",
        "  den = len(s1.union(s2))\n",
        "\n",
        "  if den == 0: \n",
        "    return 0\n",
        "  return num/den"
      ],
      "metadata": {
        "id": "Za6XRLwIwx0g"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Similarity-based Rating Estimation"
      ],
      "metadata": {
        "id": "YfBJorNy4WTx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def MSE(predictions, labels):\n",
        "    differences = [(x-y)**2 for x,y in zip(predictions,labels)]\n",
        "    return sum(differences) / len(differences)"
      ],
      "metadata": {
        "id": "W05IqlRE2Eh7"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predictRating(user, item):\n",
        "  ratings = []\n",
        "  similarities = []\n",
        "\n",
        "  for d in reviewsPerUser[user]:\n",
        "    item2 = d['item']\n",
        "\n",
        "    if item2 == item: continue\n",
        "\n",
        "    if(itemAverages[item2] != 0):\n",
        "      ratings.append(d['rating'] - itemAverages[item2])\n",
        "    else:\n",
        "      ratings.append(d['rating'] - ratingMean)\n",
        "\n",
        "    similarities.append(jaccard(usersPerItem[item2], usersPerItem[item]))\n",
        "\n",
        "  if (sum(similarities) > 0):\n",
        "    weightedRatings = [(x*y) for x,y in zip(ratings,similarities)]\n",
        "    if(itemAverages[item] != 0):\n",
        "      return itemAverages[item] + sum(weightedRatings) / sum(similarities)\n",
        "    else:\n",
        "      return ratingMean + sum(weightedRatings) / sum(similarities)\n",
        "  else:\n",
        "    if(itemAverages[item] != 0):\n",
        "      return itemAverages[item]\n",
        "    else:\n",
        "      return ratingMean"
      ],
      "metadata": {
        "id": "ncU7b1WKvoSH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "alwaysMean = [] # when you always predict mean\n",
        "predictions = []\n",
        "actual = []\n",
        "\n",
        "for index, row in test_dataset.iterrows():\n",
        "  user = row['user']\n",
        "  item = row['item']\n",
        "  actual_rating = row['rating']\n",
        "  predicted_rating = predictRating(user, item)\n",
        "\n",
        "  actual.append(actual_rating)\n",
        "  alwaysMean.append(ratingMean)\n",
        "  predictions.append(predicted_rating)"
      ],
      "metadata": {
        "id": "OXPR-XVN1dE1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MSE(alwaysMean, actual)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odg5FNWD2LZ2",
        "outputId": "df6a6ee0-d84c-48ec-c733-aba753ca268c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.2220352124670564"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "MSE(predictions, actual)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9zWXG_7i2G6R",
        "outputId": "ca778cca-36c1-44cb-92d8-23e1c9cb0d3b"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.9520315086437812"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}