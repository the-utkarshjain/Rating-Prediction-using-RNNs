{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jtj0AssZEM9U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "515a9bbc-66ca-466c-b9e6-696d37e82caf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import utils\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.vocab import GloVe, vocab, build_vocab_from_iterator\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "def yield_tokens(filereader):\n",
        "    for rows in filereader:\n",
        "      review = rows[5]\n",
        "      tokens = tokenizer(review)\n",
        "      yield tokens\n",
        "\n",
        "def build_vocab_from_dataset(dataset_path = '/content/drive/Shareddrives/CSE258/clean_data/train.csv', min_freq = 5):\n",
        "    csvfile = open(dataset_path, newline='')\n",
        "    filereader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n",
        "    column_names = next(filereader)\n",
        "    \n",
        "    vocabulary = build_vocab_from_iterator(yield_tokens(filereader), min_freq = min_freq, specials=[\"<unk>\"])\n",
        "    vocabulary.set_default_index(vocabulary[\"<unk>\"])\n",
        "\n",
        "    return vocabulary"
      ],
      "metadata": {
        "id": "jRIRKhgxFR6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = build_vocab_from_dataset()"
      ],
      "metadata": {
        "id": "lp0T7eT3Q6Fh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class reviewsDataset(Dataset):\n",
        "  def __init__(self, csv_file):\n",
        "    self.data = pd.read_csv(csv_file)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    review = self.data.iloc[idx]['review']\n",
        "    rating = self.data.iloc[idx]['rating']\n",
        "\n",
        "    return {'review': str(review), 'rating': rating}"
      ],
      "metadata": {
        "id": "MCDazl8NO61e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/drive/Shareddrives/CSE258/clean_data/train.csv'\n",
        "\n",
        "def collate_fn(data):\n",
        "  reviews = []\n",
        "  ratings = []\n",
        "  length = []\n",
        "\n",
        "  for d in data:\n",
        "    review = d['review']\n",
        "    rating = d['rating']\n",
        "\n",
        "    tokens = vocabulary(tokenizer(review))\n",
        "    reviews.append(torch.tensor(tokens, dtype = torch.long, device = device))\n",
        "    length.append(len(tokens))\n",
        "    ratings.append(rating)\n",
        "  \n",
        "  ratings = torch.tensor(ratings, dtype = torch.float, device = device)\n",
        "  return reviews, ratings, length\n",
        "\n",
        "dataset = reviewsDataset(dataset_path)\n",
        "dataloader = DataLoader(dataset, batch_size = 64, collate_fn = collate_fn)"
      ],
      "metadata": {
        "id": "x7-QwEEtPlei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, vocab_size, hidden_size):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    self.embedding = nn.Embedding(self.vocab_size, self.hidden_size)\n",
        "\n",
        "    self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, batch_first = True)\n",
        "    self.linear1 = nn.Linear(self.hidden_size, int(self.hidden_size/2))\n",
        "    self.linear2 = nn.Linear(int(self.hidden_size/2), 1)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, input):\n",
        "    embeddings = []\n",
        "    for r in input:\n",
        "      embeddings.append(self.embedding(r))\n",
        "    packed_embeddings = utils.rnn.pack_sequence(embeddings, enforce_sorted = False)\n",
        "\n",
        "    output, (h_n,c_n) = self.lstm(packed_embeddings)\n",
        "    h_n_linear1 = self.relu(self.linear1(h_n))\n",
        "    h_n_linear2 = self.linear2(h_n_linear1)\n",
        "    h_n_linear2 = h_n_linear2.squeeze(0).squeeze(-1)\n",
        "\n",
        "    return h_n_linear2\n",
        "\n",
        "lstm = LSTM(len(vocabulary), 256).to(device)"
      ],
      "metadata": {
        "id": "7uDegExEQf55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLSTM(nn.Module):\n",
        "  def __init__(self, vocab_size, hidden_size):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    self.embedding = nn.Embedding(self.vocab_size, self.hidden_size)\n",
        "\n",
        "    self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, batch_first = True, bidirectional = True)\n",
        "    self.linear1 = nn.Linear(2*self.hidden_size, self.hidden_size)\n",
        "    self.linear2 = nn.Linear(self.hidden_size, 1)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, input):\n",
        "    embeddings = []\n",
        "    for r in input:\n",
        "      embeddings.append(self.embedding(r))\n",
        "    packed_embeddings = utils.rnn.pack_sequence(embeddings, enforce_sorted = False)\n",
        "\n",
        "    output, (h_n,c_n) = self.lstm(packed_embeddings)\n",
        "    h_n = torch.cat((h_n[0], h_n[1]), dim = 1)\n",
        "    h_n_linear1 = self.relu(self.linear1(h_n))\n",
        "    h_n_linear2 = self.linear2(h_n_linear1)\n",
        "    h_n_linear2 = h_n_linear2.squeeze(0).squeeze(-1)\n",
        "\n",
        "    return h_n_linear2\n",
        "\n",
        "bilstm = BiLSTM(len(vocabulary), 256).to(device)"
      ],
      "metadata": {
        "id": "IIOJ9uS5EICs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_dataset_path = '/content/drive/Shareddrives/CSE258/clean_data/valid.csv'\n",
        "valid_dataset = reviewsDataset(valid_dataset_path)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size = 64, collate_fn = collate_fn)"
      ],
      "metadata": {
        "id": "HFmXmrWokJtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validation_loss(dataset, dataloader, model, criterion):\n",
        "  model.eval()\n",
        "  valid_loss = 0\n",
        "\n",
        "  for i, (packed_sequence, ratings, _) in enumerate(dataloader):\n",
        "    prediction = model(packed_sequence)\n",
        "    valid_loss += criterion(prediction, ratings)\n",
        "\n",
        "  return valid_loss/len(dataset)\n",
        "\n",
        "def train(input, target, model, optimizer, criterion):\n",
        "  prediction = model(input)\n",
        "  loss = criterion(prediction, target)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  return loss/target.shape[0]\n",
        "\n",
        "def train_iter(model, epochs, print_every = 244, lr = 0.001):\n",
        "  optimizer = optim.Adam(model.parameters(), lr = lr)\n",
        "  criterion = nn.MSELoss(reduction = 'sum')\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    loss_per_epoch = 0\n",
        "    loss_per_print = 0\n",
        "\n",
        "    for i, (packed_sequence, ratings, _) in enumerate(dataloader):\n",
        "      loss = train(packed_sequence, ratings, model, optimizer, criterion)\n",
        "      loss_per_epoch += loss\n",
        "      loss_per_print += loss\n",
        "\n",
        "      if (i+1)%print_every == 0:\n",
        "        print(f\"Epoch : {epoch+1}/{epochs}, i: {i+1}/{len(dataloader)}, Training Loss: {loss_per_print/(print_every)}\")\n",
        "        loss_per_print = 0\n",
        "    \n",
        "    print(\"==============================\")\n",
        "    print(f\"Epoch {epoch+1} Summary\")\n",
        "    print(f\"Training Loss: {loss_per_epoch/len(dataloader)}\")\n",
        "    valid_loss = validation_loss(valid_dataset, valid_dataloader, model, criterion)\n",
        "    print(f\"Validation Loss: {valid_loss}\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "train_iter(lstm, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zi_4BhjQY5bZ",
        "outputId": "0a514a44-3bb8-4945-c938-08a1aa5aaaec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 1/10, i: 244/2446, Training Loss: 2.8056352138519287\n",
            "Epoch : 1/10, i: 488/2446, Training Loss: 1.768242359161377\n",
            "Epoch : 1/10, i: 732/2446, Training Loss: 1.5090004205703735\n",
            "Epoch : 1/10, i: 976/2446, Training Loss: 1.3660022020339966\n",
            "Epoch : 1/10, i: 1220/2446, Training Loss: 1.3121775388717651\n",
            "Epoch : 1/10, i: 1464/2446, Training Loss: 1.28831148147583\n",
            "Epoch : 1/10, i: 1708/2446, Training Loss: 1.251430630683899\n",
            "Epoch : 1/10, i: 1952/2446, Training Loss: 1.2287451028823853\n",
            "Epoch : 1/10, i: 2196/2446, Training Loss: 1.1871510744094849\n",
            "Epoch : 1/10, i: 2440/2446, Training Loss: 1.166727066040039\n",
            "==============================\n",
            "Epoch 1 Summary\n",
            "Training Loss: 1.4877877235412598\n",
            "Validation Loss: 1.1819641590118408\n",
            "==============================\n",
            "Epoch : 2/10, i: 244/2446, Training Loss: 1.1189104318618774\n",
            "Epoch : 2/10, i: 488/2446, Training Loss: 1.0574989318847656\n",
            "Epoch : 2/10, i: 732/2446, Training Loss: 1.081725001335144\n",
            "Epoch : 2/10, i: 976/2446, Training Loss: 1.0412999391555786\n",
            "Epoch : 2/10, i: 1220/2446, Training Loss: 1.0252785682678223\n",
            "Epoch : 2/10, i: 1464/2446, Training Loss: 1.0398083925247192\n",
            "Epoch : 2/10, i: 1708/2446, Training Loss: 1.0197631120681763\n",
            "Epoch : 2/10, i: 1952/2446, Training Loss: 1.0154731273651123\n",
            "Epoch : 2/10, i: 2196/2446, Training Loss: 0.9724518656730652\n",
            "Epoch : 2/10, i: 2440/2446, Training Loss: 0.9677794575691223\n",
            "==============================\n",
            "Epoch 2 Summary\n",
            "Training Loss: 1.0339916944503784\n",
            "Validation Loss: 1.1132495403289795\n",
            "==============================\n",
            "Epoch : 3/10, i: 244/2446, Training Loss: 0.9344263672828674\n",
            "Epoch : 3/10, i: 488/2446, Training Loss: 0.8847724795341492\n",
            "Epoch : 3/10, i: 732/2446, Training Loss: 0.8970884680747986\n",
            "Epoch : 3/10, i: 976/2446, Training Loss: 0.8679258823394775\n",
            "Epoch : 3/10, i: 1220/2446, Training Loss: 0.8581613302230835\n",
            "Epoch : 3/10, i: 1464/2446, Training Loss: 0.8675188422203064\n",
            "Epoch : 3/10, i: 1708/2446, Training Loss: 0.8564630150794983\n",
            "Epoch : 3/10, i: 1952/2446, Training Loss: 0.8402977585792542\n",
            "Epoch : 3/10, i: 2196/2446, Training Loss: 0.805072546005249\n",
            "Epoch : 3/10, i: 2440/2446, Training Loss: 0.7968734502792358\n",
            "==============================\n",
            "Epoch 3 Summary\n",
            "Training Loss: 0.8607695698738098\n",
            "Validation Loss: 1.1347272396087646\n",
            "==============================\n",
            "Epoch : 4/10, i: 244/2446, Training Loss: 0.7713878750801086\n",
            "Epoch : 4/10, i: 488/2446, Training Loss: 0.7281606793403625\n",
            "Epoch : 4/10, i: 732/2446, Training Loss: 0.7234590649604797\n",
            "Epoch : 4/10, i: 976/2446, Training Loss: 0.6930273175239563\n",
            "Epoch : 4/10, i: 1220/2446, Training Loss: 0.6921815872192383\n",
            "Epoch : 4/10, i: 1464/2446, Training Loss: 0.706264853477478\n",
            "Epoch : 4/10, i: 1708/2446, Training Loss: 0.6997381448745728\n",
            "Epoch : 4/10, i: 1952/2446, Training Loss: 0.6782996654510498\n",
            "Epoch : 4/10, i: 2196/2446, Training Loss: 0.6459398865699768\n",
            "Epoch : 4/10, i: 2440/2446, Training Loss: 0.6332844495773315\n",
            "==============================\n",
            "Epoch 4 Summary\n",
            "Training Loss: 0.6970717906951904\n",
            "Validation Loss: 1.1941791772842407\n",
            "==============================\n",
            "Epoch : 5/10, i: 244/2446, Training Loss: 0.6140975952148438\n",
            "Epoch : 5/10, i: 488/2446, Training Loss: 0.5816587805747986\n",
            "Epoch : 5/10, i: 732/2446, Training Loss: 0.5761086940765381\n",
            "Epoch : 5/10, i: 976/2446, Training Loss: 0.5532501935958862\n",
            "Epoch : 5/10, i: 1220/2446, Training Loss: 0.5480101704597473\n",
            "Epoch : 5/10, i: 1464/2446, Training Loss: 0.5600523948669434\n",
            "Epoch : 5/10, i: 1708/2446, Training Loss: 0.5554596781730652\n",
            "Epoch : 5/10, i: 1952/2446, Training Loss: 0.5353386402130127\n",
            "Epoch : 5/10, i: 2196/2446, Training Loss: 0.5172242522239685\n",
            "Epoch : 5/10, i: 2440/2446, Training Loss: 0.5036039352416992\n",
            "==============================\n",
            "Epoch 5 Summary\n",
            "Training Loss: 0.5544299483299255\n",
            "Validation Loss: 1.2448729276657104\n",
            "==============================\n",
            "Epoch : 6/10, i: 244/2446, Training Loss: 0.50560462474823\n",
            "Epoch : 6/10, i: 488/2446, Training Loss: 0.47719067335128784\n",
            "Epoch : 6/10, i: 732/2446, Training Loss: 0.4687909781932831\n",
            "Epoch : 6/10, i: 976/2446, Training Loss: 0.4502634108066559\n",
            "Epoch : 6/10, i: 1220/2446, Training Loss: 0.43898898363113403\n",
            "Epoch : 6/10, i: 1464/2446, Training Loss: 0.46568405628204346\n",
            "Epoch : 6/10, i: 1708/2446, Training Loss: 0.4513363838195801\n",
            "Epoch : 6/10, i: 1952/2446, Training Loss: 0.4434662163257599\n",
            "Epoch : 6/10, i: 2196/2446, Training Loss: 0.426677405834198\n",
            "Epoch : 6/10, i: 2440/2446, Training Loss: 0.4272814691066742\n",
            "==============================\n",
            "Epoch 6 Summary\n",
            "Training Loss: 0.4555623531341553\n",
            "Validation Loss: 1.2765752077102661\n",
            "==============================\n",
            "Epoch : 7/10, i: 244/2446, Training Loss: 0.42591485381126404\n",
            "Epoch : 7/10, i: 488/2446, Training Loss: 0.40435779094696045\n",
            "Epoch : 7/10, i: 732/2446, Training Loss: 0.39988425374031067\n",
            "Epoch : 7/10, i: 976/2446, Training Loss: 0.3837590217590332\n",
            "Epoch : 7/10, i: 1220/2446, Training Loss: 0.37262362241744995\n",
            "Epoch : 7/10, i: 1464/2446, Training Loss: 0.3848485052585602\n",
            "Epoch : 7/10, i: 1708/2446, Training Loss: 0.3825836181640625\n",
            "Epoch : 7/10, i: 1952/2446, Training Loss: 0.3905937969684601\n",
            "Epoch : 7/10, i: 2196/2446, Training Loss: 0.3573451638221741\n",
            "Epoch : 7/10, i: 2440/2446, Training Loss: 0.3762190341949463\n",
            "==============================\n",
            "Epoch 7 Summary\n",
            "Training Loss: 0.38787075877189636\n",
            "Validation Loss: 1.3257815837860107\n",
            "==============================\n",
            "Epoch : 8/10, i: 244/2446, Training Loss: 0.36959153413772583\n",
            "Epoch : 8/10, i: 488/2446, Training Loss: 0.35634350776672363\n",
            "Epoch : 8/10, i: 732/2446, Training Loss: 0.34182366728782654\n",
            "Epoch : 8/10, i: 976/2446, Training Loss: 0.34161314368247986\n",
            "Epoch : 8/10, i: 1220/2446, Training Loss: 0.3520187735557556\n",
            "Epoch : 8/10, i: 1464/2446, Training Loss: 0.3464289605617523\n",
            "Epoch : 8/10, i: 1708/2446, Training Loss: 0.34596481919288635\n",
            "Epoch : 8/10, i: 1952/2446, Training Loss: 0.3398694097995758\n",
            "Epoch : 8/10, i: 2196/2446, Training Loss: 0.319721519947052\n",
            "Epoch : 8/10, i: 2440/2446, Training Loss: 0.32657724618911743\n",
            "==============================\n",
            "Epoch 8 Summary\n",
            "Training Loss: 0.34400585293769836\n",
            "Validation Loss: 1.3484054803848267\n",
            "==============================\n",
            "Epoch : 9/10, i: 244/2446, Training Loss: 0.31721505522727966\n",
            "Epoch : 9/10, i: 488/2446, Training Loss: 0.32017460465431213\n",
            "Epoch : 9/10, i: 732/2446, Training Loss: 0.3062232732772827\n",
            "Epoch : 9/10, i: 976/2446, Training Loss: 0.2886990010738373\n",
            "Epoch : 9/10, i: 1220/2446, Training Loss: 0.30750423669815063\n",
            "Epoch : 9/10, i: 1464/2446, Training Loss: 0.29950523376464844\n",
            "Epoch : 9/10, i: 1708/2446, Training Loss: 0.3076845407485962\n",
            "Epoch : 9/10, i: 1952/2446, Training Loss: 0.292967289686203\n",
            "Epoch : 9/10, i: 2196/2446, Training Loss: 0.2911278009414673\n",
            "Epoch : 9/10, i: 2440/2446, Training Loss: 0.28335896134376526\n",
            "==============================\n",
            "Epoch 9 Summary\n",
            "Training Loss: 0.30145663022994995\n",
            "Validation Loss: 1.345595359802246\n",
            "==============================\n",
            "Epoch : 10/10, i: 244/2446, Training Loss: 0.27891841530799866\n",
            "Epoch : 10/10, i: 488/2446, Training Loss: 0.2794923186302185\n",
            "Epoch : 10/10, i: 732/2446, Training Loss: 0.263590008020401\n",
            "Epoch : 10/10, i: 976/2446, Training Loss: 0.2558995187282562\n",
            "Epoch : 10/10, i: 1220/2446, Training Loss: 0.26413917541503906\n",
            "Epoch : 10/10, i: 1464/2446, Training Loss: 0.2634212076663971\n",
            "Epoch : 10/10, i: 1708/2446, Training Loss: 0.2749362289905548\n",
            "Epoch : 10/10, i: 1952/2446, Training Loss: 0.267785906791687\n",
            "Epoch : 10/10, i: 2196/2446, Training Loss: 0.26503127813339233\n",
            "Epoch : 10/10, i: 2440/2446, Training Loss: 0.24897578358650208\n",
            "==============================\n",
            "Epoch 10 Summary\n",
            "Training Loss: 0.26615703105926514\n",
            "Validation Loss: 1.3475133180618286\n",
            "==============================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lstm = LSTM(len(vocabulary), 256).to(device)\n",
        "train_iter(lstm, 2)\n",
        "\n",
        "test_dataset_path = '/content/drive/Shareddrives/CSE258/clean_data/test.csv'\n",
        "test_dataset = reviewsDataset(test_dataset_path)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size = 64, collate_fn = collate_fn)\n",
        "\n",
        "def test_loss(dataset, dataloader, model, criterion):\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "\n",
        "  for i, (packed_sequence, ratings, _) in enumerate(dataloader):\n",
        "    prediction = model(packed_sequence)\n",
        "    test_loss += criterion(prediction, ratings)\n",
        "\n",
        "  return test_loss/len(dataset)\n",
        "\n",
        "test_loss(test_dataset, test_dataloader, lstm, nn.MSELoss(reduction = 'sum'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZaT4rPY-25S4",
        "outputId": "83698cab-c652-4e32-8e6f-803c360273a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.1106, device='cuda:0', grad_fn=<DivBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "flsVIpNA5z6S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}