{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Jtj0AssZEM9U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a567b42-f5f7-43e8-dd06-f1d5100b6ce9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import utils\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.vocab import GloVe, vocab, build_vocab_from_iterator\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "\n",
        "glove = GloVe(name = \"6B\", dim = 200, cache = \"/content/drive/Shareddrives/CSE258/\")"
      ],
      "metadata": {
        "id": "jRIRKhgxFR6d"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = vocab(glove.stoi, min_freq = 0)\n",
        "pretrained_weights = glove.vectors\n",
        "\n",
        "unk_idx = len(vocabulary)\n",
        "vocabulary.insert_token(\"<unk>\", unk_idx)\n",
        "vocabulary.set_default_index(unk_idx)"
      ],
      "metadata": {
        "id": "lp0T7eT3Q6Fh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class reviewsDataset(Dataset):\n",
        "  def __init__(self, csv_file):\n",
        "    self.data = pd.read_csv(csv_file)\n",
        "  \n",
        "  def __len__(self):\n",
        "    return len(self.data)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    review = self.data.iloc[idx]['review']\n",
        "    rating = self.data.iloc[idx]['rating']\n",
        "\n",
        "    return {'review': str(review), 'rating': rating}"
      ],
      "metadata": {
        "id": "MCDazl8NO61e"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(data):\n",
        "  reviews = []\n",
        "  ratings = []\n",
        "  length = []\n",
        "\n",
        "  for d in data:\n",
        "    review = d['review']\n",
        "    rating = d['rating']\n",
        "\n",
        "    tokens = vocabulary(tokenizer(review))\n",
        "    tokens = [t for t in tokens if t != 400000]\n",
        "    if(len(tokens) > 0):\n",
        "      reviews.append(torch.tensor(tokens, dtype = torch.long, device = device))\n",
        "      length.append(len(tokens))\n",
        "      ratings.append(rating)\n",
        "  \n",
        "  ratings = torch.tensor(ratings, dtype = torch.float, device = device)\n",
        "  return reviews, ratings, length"
      ],
      "metadata": {
        "id": "x7-QwEEtPlei"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "  def __init__(self, weight):\n",
        "    super().__init__()\n",
        "    self.hidden_size = weight.shape[-1]\n",
        "\n",
        "    self.embedding = nn.Embedding.from_pretrained(weight, freeze = True)\n",
        "\n",
        "    self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, batch_first = True)\n",
        "    self.linear1 = nn.Linear(self.hidden_size, int(self.hidden_size/2))\n",
        "    self.dropout = nn.Dropout(p=0.6)\n",
        "    self.linear2 = nn.Linear(int(self.hidden_size/2), 1)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, input):\n",
        "    embeddings = []\n",
        "    for r in input:\n",
        "      embeddings.append(self.embedding(r))\n",
        "    packed_embeddings = utils.rnn.pack_sequence(embeddings, enforce_sorted = False)\n",
        "\n",
        "    output, (h_n,c_n) = self.lstm(packed_embeddings)\n",
        "    h_n_linear1 = self.dropout(self.relu(self.linear1(h_n)))\n",
        "    h_n_linear2 = self.linear2(h_n_linear1)\n",
        "    h_n_linear2 = h_n_linear2.squeeze(0).squeeze(-1)\n",
        "\n",
        "    return h_n_linear2\n",
        "\n",
        "lstm = LSTM(pretrained_weights).to(device)"
      ],
      "metadata": {
        "id": "7uDegExEQf55"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BiLSTM(nn.Module):\n",
        "  def __init__(self, vocab_size, hidden_size):\n",
        "    super().__init__()\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hidden_size = hidden_size\n",
        "\n",
        "    self.embedding = nn.Embedding(self.vocab_size, self.hidden_size)\n",
        "\n",
        "    self.lstm = nn.LSTM(self.hidden_size, self.hidden_size, batch_first = True, bidirectional = True)\n",
        "    self.linear1 = nn.Linear(2*self.hidden_size, self.hidden_size)\n",
        "    self.linear2 = nn.Linear(self.hidden_size, 1)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "  def forward(self, input):\n",
        "    embeddings = []\n",
        "    for r in input:\n",
        "      embeddings.append(self.embedding(r))\n",
        "    packed_embeddings = utils.rnn.pack_sequence(embeddings, enforce_sorted = False)\n",
        "\n",
        "    output, (h_n,c_n) = self.lstm(packed_embeddings)\n",
        "    h_n = torch.cat((h_n[0], h_n[1]), dim = 1)\n",
        "    h_n_linear1 = self.relu(self.linear1(h_n))\n",
        "    h_n_linear2 = self.linear2(h_n_linear1)\n",
        "    h_n_linear2 = h_n_linear2.squeeze(0).squeeze(-1)\n",
        "\n",
        "    return h_n_linear2\n",
        "\n",
        "bilstm = BiLSTM(pretrained_weights).to(device)"
      ],
      "metadata": {
        "id": "IIOJ9uS5EICs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_path = '/content/drive/Shareddrives/CSE258/clean_data/train.csv'\n",
        "dataset = reviewsDataset(dataset_path)\n",
        "dataloader = DataLoader(dataset, batch_size = 64, collate_fn = collate_fn)\n",
        "\n",
        "valid_dataset_path = '/content/drive/Shareddrives/CSE258/clean_data/valid.csv'\n",
        "valid_dataset = reviewsDataset(valid_dataset_path)\n",
        "valid_dataloader = DataLoader(valid_dataset, batch_size = 64, collate_fn = collate_fn)\n",
        "\n",
        "test_dataset_path = '/content/drive/Shareddrives/CSE258/clean_data/test.csv'\n",
        "test_dataset = reviewsDataset(test_dataset_path)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size = 64, collate_fn = collate_fn)"
      ],
      "metadata": {
        "id": "HFmXmrWokJtT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_loss(dataset, dataloader, model, criterion):\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "\n",
        "  for i, (packed_sequence, ratings, _) in enumerate(dataloader):\n",
        "    prediction = model(packed_sequence)\n",
        "    test_loss += criterion(prediction, ratings)\n",
        "\n",
        "  return test_loss/len(dataset)\n",
        "\n",
        "def validation_loss(dataset, dataloader, model, criterion):\n",
        "  model.eval()\n",
        "  valid_loss = 0\n",
        "\n",
        "  for i, (packed_sequence, ratings, _) in enumerate(dataloader):\n",
        "    prediction = model(packed_sequence)\n",
        "    valid_loss += criterion(prediction, ratings)\n",
        "\n",
        "  return valid_loss/len(dataset)\n",
        "\n",
        "def train(input, target, model, optimizer, criterion):\n",
        "  prediction = model(input)\n",
        "  loss = criterion(prediction, target)\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  return loss/target.shape[0]\n",
        "\n",
        "def train_iter(model, epochs, print_every = 244, lr = 0.001):\n",
        "  optimizer = optim.Adam(model.parameters(), lr = lr)\n",
        "  criterion = nn.MSELoss(reduction = 'sum')\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    model.train()\n",
        "    loss_per_epoch = 0\n",
        "    loss_per_print = 0\n",
        "\n",
        "    for i, (packed_sequence, ratings, _) in enumerate(dataloader):\n",
        "      loss = train(packed_sequence, ratings, model, optimizer, criterion)\n",
        "      loss_per_epoch += loss\n",
        "      loss_per_print += loss\n",
        "\n",
        "      if (i+1)%print_every == 0:\n",
        "        print(f\"Epoch : {epoch+1}/{epochs}, i: {i+1}/{len(dataloader)}, Training Loss: {loss_per_print/(print_every)}\")\n",
        "        loss_per_print = 0\n",
        "    \n",
        "    print(\"==============================\")\n",
        "    print(f\"Epoch {epoch+1} Summary\")\n",
        "    print(f\"Training Loss: {loss_per_epoch/len(dataloader)}\")\n",
        "    valid_loss = validation_loss(valid_dataset, valid_dataloader, model, criterion)\n",
        "    print(f\"Validation Loss: {valid_loss}\")\n",
        "    testing_loss = test_loss(test_dataset, test_dataloader, model, criterion)\n",
        "    print(f\"Test Loss: {testing_loss}\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "train_iter(lstm, 10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zi_4BhjQY5bZ",
        "outputId": "703fd6db-ffb7-4a19-b945-c0a60ec001af"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 1/10, i: 244/2446, Training Loss: 3.051501750946045\n",
            "Epoch : 1/10, i: 488/2446, Training Loss: 2.544358491897583\n",
            "Epoch : 1/10, i: 732/2446, Training Loss: 2.1468088626861572\n",
            "Epoch : 1/10, i: 976/2446, Training Loss: 1.8294475078582764\n",
            "Epoch : 1/10, i: 1220/2446, Training Loss: 1.8028931617736816\n",
            "Epoch : 1/10, i: 1464/2446, Training Loss: 1.740801215171814\n",
            "Epoch : 1/10, i: 1708/2446, Training Loss: 1.7076895236968994\n",
            "Epoch : 1/10, i: 1952/2446, Training Loss: 1.6558178663253784\n",
            "Epoch : 1/10, i: 2196/2446, Training Loss: 1.6239523887634277\n",
            "Epoch : 1/10, i: 2440/2446, Training Loss: 1.5827375650405884\n",
            "==============================\n",
            "Epoch 1 Summary\n",
            "Training Loss: 1.9680137634277344\n",
            "Validation Loss: 1.3882640600204468\n",
            "Test Loss: 1.4040980339050293\n",
            "==============================\n",
            "Epoch : 2/10, i: 244/2446, Training Loss: 1.535567045211792\n",
            "Epoch : 2/10, i: 488/2446, Training Loss: 1.4973684549331665\n",
            "Epoch : 2/10, i: 732/2446, Training Loss: 1.5110254287719727\n",
            "Epoch : 2/10, i: 976/2446, Training Loss: 1.4281284809112549\n",
            "Epoch : 2/10, i: 1220/2446, Training Loss: 1.4398517608642578\n",
            "Epoch : 2/10, i: 1464/2446, Training Loss: 1.425849199295044\n",
            "Epoch : 2/10, i: 1708/2446, Training Loss: 1.44871187210083\n",
            "Epoch : 2/10, i: 1952/2446, Training Loss: 1.3809444904327393\n",
            "Epoch : 2/10, i: 2196/2446, Training Loss: 1.3592642545700073\n",
            "Epoch : 2/10, i: 2440/2446, Training Loss: 1.3305953741073608\n",
            "==============================\n",
            "Epoch 2 Summary\n",
            "Training Loss: 1.4359958171844482\n",
            "Validation Loss: 1.1999341249465942\n",
            "Test Loss: 1.2115027904510498\n",
            "==============================\n",
            "Epoch : 3/10, i: 244/2446, Training Loss: 1.2998486757278442\n",
            "Epoch : 3/10, i: 488/2446, Training Loss: 1.2959035634994507\n",
            "Epoch : 3/10, i: 732/2446, Training Loss: 1.3130254745483398\n",
            "Epoch : 3/10, i: 976/2446, Training Loss: 1.2457890510559082\n",
            "Epoch : 3/10, i: 1220/2446, Training Loss: 1.2470070123672485\n",
            "Epoch : 3/10, i: 1464/2446, Training Loss: 1.241633415222168\n",
            "Epoch : 3/10, i: 1708/2446, Training Loss: 1.266222596168518\n",
            "Epoch : 3/10, i: 1952/2446, Training Loss: 1.2126524448394775\n",
            "Epoch : 3/10, i: 2196/2446, Training Loss: 1.1964833736419678\n",
            "Epoch : 3/10, i: 2440/2446, Training Loss: 1.1833611726760864\n",
            "==============================\n",
            "Epoch 3 Summary\n",
            "Training Loss: 1.250450849533081\n",
            "Validation Loss: 1.151485800743103\n",
            "Test Loss: 1.1601641178131104\n",
            "==============================\n",
            "Epoch : 4/10, i: 244/2446, Training Loss: 1.180930495262146\n",
            "Epoch : 4/10, i: 488/2446, Training Loss: 1.1732393503189087\n",
            "Epoch : 4/10, i: 732/2446, Training Loss: 1.1845368146896362\n",
            "Epoch : 4/10, i: 976/2446, Training Loss: 1.1348545551300049\n",
            "Epoch : 4/10, i: 1220/2446, Training Loss: 1.1399004459381104\n",
            "Epoch : 4/10, i: 1464/2446, Training Loss: 1.1369950771331787\n",
            "Epoch : 4/10, i: 1708/2446, Training Loss: 1.1462711095809937\n",
            "Epoch : 4/10, i: 1952/2446, Training Loss: 1.130021572113037\n",
            "Epoch : 4/10, i: 2196/2446, Training Loss: 1.1034053564071655\n",
            "Epoch : 4/10, i: 2440/2446, Training Loss: 1.0916904211044312\n",
            "==============================\n",
            "Epoch 4 Summary\n",
            "Training Loss: 1.1425702571868896\n",
            "Validation Loss: 1.100021243095398\n",
            "Test Loss: 1.1171084642410278\n",
            "==============================\n",
            "Epoch : 5/10, i: 244/2446, Training Loss: 1.095305323600769\n",
            "Epoch : 5/10, i: 488/2446, Training Loss: 1.0732325315475464\n",
            "Epoch : 5/10, i: 732/2446, Training Loss: 1.0932189226150513\n",
            "Epoch : 5/10, i: 976/2446, Training Loss: 1.0510984659194946\n",
            "Epoch : 5/10, i: 1220/2446, Training Loss: 1.0535542964935303\n",
            "Epoch : 5/10, i: 1464/2446, Training Loss: 1.0551791191101074\n",
            "Epoch : 5/10, i: 1708/2446, Training Loss: 1.0627843141555786\n",
            "Epoch : 5/10, i: 1952/2446, Training Loss: 1.0426281690597534\n",
            "Epoch : 5/10, i: 2196/2446, Training Loss: 1.0219556093215942\n",
            "Epoch : 5/10, i: 2440/2446, Training Loss: 1.00278902053833\n",
            "==============================\n",
            "Epoch 5 Summary\n",
            "Training Loss: 1.0555126667022705\n",
            "Validation Loss: 1.0937304496765137\n",
            "Test Loss: 1.1051474809646606\n",
            "==============================\n",
            "Epoch : 6/10, i: 244/2446, Training Loss: 1.010642170906067\n",
            "Epoch : 6/10, i: 488/2446, Training Loss: 0.9990757703781128\n",
            "Epoch : 6/10, i: 732/2446, Training Loss: 1.018548607826233\n",
            "Epoch : 6/10, i: 976/2446, Training Loss: 0.9804431796073914\n",
            "Epoch : 6/10, i: 1220/2446, Training Loss: 0.9897570610046387\n",
            "Epoch : 6/10, i: 1464/2446, Training Loss: 0.9779433012008667\n",
            "Epoch : 6/10, i: 1708/2446, Training Loss: 0.9950234889984131\n",
            "Epoch : 6/10, i: 1952/2446, Training Loss: 0.9642102718353271\n",
            "Epoch : 6/10, i: 2196/2446, Training Loss: 0.9326574802398682\n",
            "Epoch : 6/10, i: 2440/2446, Training Loss: 0.9391537308692932\n",
            "==============================\n",
            "Epoch 6 Summary\n",
            "Training Loss: 0.9808752536773682\n",
            "Validation Loss: 1.1126071214675903\n",
            "Test Loss: 1.1202484369277954\n",
            "==============================\n",
            "Epoch : 7/10, i: 244/2446, Training Loss: 0.93121337890625\n",
            "Epoch : 7/10, i: 488/2446, Training Loss: 0.9231101870536804\n",
            "Epoch : 7/10, i: 732/2446, Training Loss: 0.9448952674865723\n",
            "Epoch : 7/10, i: 976/2446, Training Loss: 0.9034695625305176\n",
            "Epoch : 7/10, i: 1220/2446, Training Loss: 0.9155324101448059\n",
            "Epoch : 7/10, i: 1464/2446, Training Loss: 0.8942960500717163\n",
            "Epoch : 7/10, i: 1708/2446, Training Loss: 0.9274209141731262\n",
            "Epoch : 7/10, i: 1952/2446, Training Loss: 0.9075184464454651\n",
            "Epoch : 7/10, i: 2196/2446, Training Loss: 0.8744617700576782\n",
            "Epoch : 7/10, i: 2440/2446, Training Loss: 0.8594036102294922\n",
            "==============================\n",
            "Epoch 7 Summary\n",
            "Training Loss: 0.9081364274024963\n",
            "Validation Loss: 1.118383765220642\n",
            "Test Loss: 1.1282286643981934\n",
            "==============================\n",
            "Epoch : 8/10, i: 244/2446, Training Loss: 0.86563640832901\n",
            "Epoch : 8/10, i: 488/2446, Training Loss: 0.872227668762207\n",
            "Epoch : 8/10, i: 732/2446, Training Loss: 0.8707506656646729\n",
            "Epoch : 8/10, i: 976/2446, Training Loss: 0.843376636505127\n",
            "Epoch : 8/10, i: 1220/2446, Training Loss: 0.8435803651809692\n",
            "Epoch : 8/10, i: 1464/2446, Training Loss: 0.8409428000450134\n",
            "Epoch : 8/10, i: 1708/2446, Training Loss: 0.8635194897651672\n",
            "Epoch : 8/10, i: 1952/2446, Training Loss: 0.839590311050415\n",
            "Epoch : 8/10, i: 2196/2446, Training Loss: 0.8148893713951111\n",
            "Epoch : 8/10, i: 2440/2446, Training Loss: 0.7982768416404724\n",
            "==============================\n",
            "Epoch 8 Summary\n",
            "Training Loss: 0.8451257944107056\n",
            "Validation Loss: 1.1891874074935913\n",
            "Test Loss: 1.1987721920013428\n",
            "==============================\n",
            "Epoch : 9/10, i: 244/2446, Training Loss: 0.8110541105270386\n",
            "Epoch : 9/10, i: 488/2446, Training Loss: 0.8004104495048523\n",
            "Epoch : 9/10, i: 732/2446, Training Loss: 0.8091105818748474\n",
            "Epoch : 9/10, i: 976/2446, Training Loss: 0.7812602519989014\n",
            "Epoch : 9/10, i: 1220/2446, Training Loss: 0.7819328904151917\n",
            "Epoch : 9/10, i: 1464/2446, Training Loss: 0.7944813966751099\n",
            "Epoch : 9/10, i: 1708/2446, Training Loss: 0.8084283471107483\n",
            "Epoch : 9/10, i: 1952/2446, Training Loss: 0.7766594886779785\n",
            "Epoch : 9/10, i: 2196/2446, Training Loss: 0.7626901268959045\n",
            "Epoch : 9/10, i: 2440/2446, Training Loss: 0.7440205812454224\n",
            "==============================\n",
            "Epoch 9 Summary\n",
            "Training Loss: 0.7869219779968262\n",
            "Validation Loss: 1.2034400701522827\n",
            "Test Loss: 1.2163329124450684\n",
            "==============================\n",
            "Epoch : 10/10, i: 244/2446, Training Loss: 0.7627191543579102\n",
            "Epoch : 10/10, i: 488/2446, Training Loss: 0.7468786239624023\n",
            "Epoch : 10/10, i: 732/2446, Training Loss: 0.7540072202682495\n",
            "Epoch : 10/10, i: 976/2446, Training Loss: 0.7278233766555786\n",
            "Epoch : 10/10, i: 1220/2446, Training Loss: 0.7264358401298523\n",
            "Epoch : 10/10, i: 1464/2446, Training Loss: 0.7329979538917542\n",
            "Epoch : 10/10, i: 1708/2446, Training Loss: 0.7387393712997437\n",
            "Epoch : 10/10, i: 1952/2446, Training Loss: 0.7255160808563232\n",
            "Epoch : 10/10, i: 2196/2446, Training Loss: 0.7067785859107971\n",
            "Epoch : 10/10, i: 2440/2446, Training Loss: 0.7079510688781738\n",
            "==============================\n",
            "Epoch 10 Summary\n",
            "Training Loss: 0.7327459454536438\n",
            "Validation Loss: 1.2211302518844604\n",
            "Test Loss: 1.233305811882019\n",
            "==============================\n"
          ]
        }
      ]
    }
  ]
}